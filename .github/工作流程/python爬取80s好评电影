from gevent import monkey
monkey.patch_all()
import requests,time,gevent
import mysql.connector
from bs4 import BeautifulSoup
from gevent.queue import Queue

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0;\
     Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}
work = Queue()
conn = mysql.connector.Connect(user='root',passwd='huantian',database='learn')
cursor = conn.cursor()

#获取每页电影的链接，加入列表
def pageN1():
    n = int(input('输入要爬取的页数：'))
    pageN1_list = []
    for i in range(1,n+1):
        url_pageN1 = 'http://www.y80s.com/movie/list/----g-p'+str(i)
        pageN1_list.append(url_pageN1)
        print('正在获取首页链接：'+ url_pageN1)
    return pageN1_list

#爬取每部电影的地址短链接，加入列表
def pageN2(urlN1):
    url_pageN2_list = []
    for url in urlN1:
        pageN1_res = requests.get(url,headers=headers)
        pageN1_soup = BeautifulSoup(pageN1_res.text,'html.parser')
        pageN1_film = pageN1_soup.findAll(class_='h3')
        for film in pageN1_film:
            url_pageN2 = 'http://www.y80s.com'+ (film.find('a')['href']).strip()
            url_pageN2_list.append(url_pageN2)
            print('正在获取电影详情页链接：'+ url_pageN2)
            time.sleep(0.3)
    return url_pageN2_list

#爬取每部电影的信息
def sprider():
    while not work.empty():
        try:
            real_url = work.get_nowait()
            pageN2_res = requests.get(real_url,headers=headers)
            pageN2_soup = BeautifulSoup(pageN2_res.text,'html.parser')
            # film_info = pageN2_soup.find('div',class_='info')
            # print(film_info)
            film_name = pageN2_soup.find('h1',class_='font14w').text.strip().replace(' ','')
            film_story = pageN2_soup.find(id='movie_content').text[:-12]
            film_score = pageN2_soup.find('div',style='float:left; margin-right:10px;').text.strip()[-3:]
            film_play_url = 'http://www.y80s.com'+ pageN2_soup.find('div',class_='info').findAll('a')[-1]['href']
            xldown_url = pageN2_soup.find(class_='xunlei dlbutton1').find('a')['href']
            print('电影名：{}\n电影简介：{}\n豆瓣评分：{}\n播放地址：{}\n迅雷下载：{}\n'.format(\
                film_name,film_story,film_score,film_play_url,xldown_url))
            sql_insert = "INSERT INTO `movie`(`name`, `story`, `score`,`play_url`, `down_url`)VALUES('%s','%s','%s','%s','%s')"%(film_name,film_story,film_score,film_play_url,xldown_url)
            print('插入语句：'+ sql_insert+'\n')
            cursor.execute(sql_insert)
            print('剩余进程：{}个...'.format(work.qsize()))
            time.sleep(9)
        except AttributeError:
            print(film_name +'爬取失败，请检查以下链接：'+real_url+'\n')

tasks_list = []
urlN1 = pageN1()
urlN2 = pageN2(urlN1)
for u in urlN2:
    # print(u)
    work.put_nowait(u)

for x in range(10):
    task = gevent.spawn(sprider)
    tasks_list.append(task)

gevent.joinall(tasks_list)
conn.commit()
conn.close()
print('爬虫结束！')
